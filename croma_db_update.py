import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from chromadb.errors import NotFoundError
import hashlib
import os
import json

# --- Ayarlar ---
CHROMA_HOST = "localhost"  # Sadece ana bilgisayar adÄ±
CHROMA_PORT = 8000  # Sadece port numarasÄ±
COLLECTION_NAME = "rag_test_data"
FILE_PATH = "/home/mbaloglu/Rag/database/"  # Kendi dosyanÄ±zÄ±n adÄ±
MAX_BATCH_SIZE = 5000


def db_update():

    # 4. ChromaDB Ä°stemcisi OluÅŸturma ve BaÄŸlanma
    client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
    print(f"Eski koleksiyon ({COLLECTION_NAME}) siliniyor...")
    try:
        # Silme iÅŸlemini dene
        client.delete_collection(COLLECTION_NAME)
        print("Silme baÅŸarÄ±lÄ±.")
    except NotFoundError:
        # EÄŸer koleksiyon yoksa (ilk Ã§alÄ±ÅŸtÄ±rma), hatayÄ± yoksay ve devam et
        print("Koleksiyon zaten mevcut deÄŸil. Yeni koleksiyon oluÅŸturulacak.")
    # 5. Chroma Veri Deposu OluÅŸturma (BaÄŸlantÄ±lÄ±)
    # VektÃ¶rler bu koleksiyona yÃ¼klenir.
    vectorstore = update_db_with_feedback(
        FILE_PATH, client=client, collection_name=COLLECTION_NAME
    )

    print(
        f"VektÃ¶rler baÅŸarÄ±yla Docker sunucusundaki '{COLLECTION_NAME}' koleksiyonuna yÃ¼klendi."
    )
    # 5. Sorgu HazÄ±rlama (Retrieval)
    return vectorstore


def create_id(chunk):
    """Metin iÃ§eriÄŸi, kaynak dosyasÄ± ve metin sÄ±rasÄ±nÄ± kullanarak benzersiz ID oluÅŸturur."""
    # Kaynak dosyasÄ± (SQuAD verisinde: dosya adÄ± + baÅŸlÄ±k)
    source = chunk.metadata.get("source", "") + chunk.metadata.get("title", "")

    # Metin iÃ§eriÄŸi
    content = chunk.page_content

    # Metin + Kaynak bilgisini birleÅŸtirip hash alÄ±yoruz
    unique_string = f"{source}_{content}"

    return hashlib.sha256(unique_string.encode("utf-8")).hexdigest()


def get_chunks_with_ids(
    file_dir_path: str, chunk_size: int = 500, chunk_overlap: int = 50
):
    """Veri dizinindeki tÃ¼m JSON dosyalarÄ±nÄ± okur, SQuAD formatÄ±nÄ± Ã§Ã¶zer ve parÃ§alara ayÄ±rÄ±r."""

    all_documents = []

    for filename in os.listdir(file_dir_path):
        if filename.endswith(".json"):
            file_path = os.path.join(file_dir_path, filename)
            print(f"-> {filename} dosyasÄ± okunuyor...")

            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # SQuAD JSON yapÄ±sÄ±nÄ± gezme
            # YapÄ±: data -> paragraphs -> context
            for item in data.get("data", []):
                for paragraph_data in item.get("paragraphs", []):
                    # Her bir paragrafÄ± (context), tÃ¼m SQuAD veri setinin ana baÄŸlamÄ±nÄ± temsil eden
                    # tek bir belge olarak alÄ±yoruz.
                    context_text = paragraph_data.get("context")
                    title = item.get("title", "Bilinmeyen BaÅŸlÄ±k")

                    if context_text:
                        # LangChain Document nesnesi oluÅŸturma
                        all_documents.append(
                            Document(
                                page_content=context_text,
                                metadata={"source": file_path, "title": title},
                            )
                        )

    # 2. BÃ¶lme (Chunking) - ArtÄ±k parÃ§alama iÅŸlemini bÃ¼yÃ¼k paragraflar Ã¼zerinde yapÄ±yoruz
    print(
        f"Toplam {len(all_documents)} paragraf/belge hazÄ±rlandÄ±. ParÃ§alara ayrÄ±lÄ±yor..."
    )
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],
    )
    all_chunks = text_splitter.split_documents(all_documents)

    # 3. ID OluÅŸturma ve AyÄ±rma
    ids = []
    for i, chunk in enumerate(all_chunks):  # enumerate ile sÄ±rasÄ±nÄ± (i) alÄ±yoruz
        # Metin iÃ§eriÄŸine sadece i (index) ekleyerek benzersizliÄŸi garanti ediyoruz
        unique_string = f"{chunk.page_content}_{i}"
        chunk_id = hashlib.sha256(unique_string.encode("utf-8")).hexdigest()

        ids.append(chunk_id)
        chunk.metadata["ids"] = chunk_id

    return all_chunks, ids


def update_db_with_feedback(
    file_dir_path: str, client: chromadb.HttpClient, collection_name: str
):
    """
    VeritabanÄ±nÄ± gÃ¼nceller, mevcut parÃ§alarÄ± kontrol eder ve geri bildirim saÄŸlar.
    (Mevcut parÃ§alar overwrite edilir, silinmez).
    """

    # Veriyi hazÄ±rlama
    all_chunks, ids = get_chunks_with_ids(file_dir_path)
    print(f"HazÄ±rlanan toplam parÃ§a sayÄ±sÄ±: {len(all_chunks)}")

    embedding_function = HuggingFaceEmbeddings(
        model_name="paraphrase-multilingual-mpnet-base-v2"
    )

    # VektÃ¶r deposunu al (EÄŸer koleksiyon yoksa oluÅŸturur)
    vectorstore = Chroma(
        client=client,
        collection_name=collection_name,
        embedding_function=embedding_function,
    )

    # ------------------ DURUM KONTROLÃœ VE YÃœKLEME ------------------

    # ChromaDB'ye parÃ§a parÃ§a ekleme/gÃ¼ncelleme (Daha fazla kontrol iÃ§in)
    # LangChain'in add_documents'u arka planda ID'ye gÃ¶re gÃ¼ncelleme (overwrite) yapar.

    # Mevcut tÃ¼m ID'leri Ã§ekelim (Sadece durum kontrolÃ¼ iÃ§in)
    try:
        existing_ids = client.get_collection(collection_name).get(include=[])["ids"]
        existing_ids_set = set(existing_ids)
    except Exception:
        # Koleksiyon ilk kez oluÅŸturuluyorsa
        existing_ids_set = set()

    chunks_to_add = []
    ids_to_add = []

    # Hangi parÃ§alarÄ±n yeni, hangilerinin gÃ¼ncelleneceÄŸini belirleme (loglama iÃ§in)
    new_count = 0
    updated_count = 0

    for i, chunk_id in enumerate(ids):
        if chunk_id in existing_ids_set:
            # ID zaten var, bu OVERWRITE (gÃ¼ncelleme) olacak
            updated_count += 1
            # GÃ¼ncelleme de ekleme iÅŸlemiyle aynÄ±dÄ±r
            chunks_to_add.append(all_chunks[i])
            ids_to_add.append(chunk_id)
        else:
            # ID yok, bu yeni bir ekleme olacak
            new_count += 1
            chunks_to_add.append(all_chunks[i])
            ids_to_add.append(chunk_id)

    # YÃ¼kleme (LangChain'in add_documents metodu hem yeni ekler hem de var olan ID'leri gÃ¼nceller)
    if chunks_to_add:
        # 1. YÃ¼kleme iÅŸlemini parÃ§alara (batch) ayÄ±rÄ±yoruz
        num_chunks = len(chunks_to_add)

        # for dÃ¶ngÃ¼sÃ¼ ile 0'dan baÅŸlayarak, MAX_BATCH_SIZE adÄ±mlarÄ±yla ilerle
        for i in range(0, num_chunks, MAX_BATCH_SIZE):

            # BaÅŸlangÄ±Ã§ ve bitiÅŸ indexlerini belirle
            end_index = min(i + MAX_BATCH_SIZE, num_chunks)

            # Chunk ve ID'leri bu batch iÃ§in ayÄ±r
            batch_chunks = chunks_to_add[i:end_index]
            batch_ids = ids_to_add[i:end_index]

            print(
                f"   -> Batch {int(i/MAX_BATCH_SIZE) + 1}: {len(batch_chunks)} parÃ§a yÃ¼kleniyor..."
            )

            try:
                # 2. Batch'i yÃ¼kle
                vectorstore.add_documents(documents=batch_chunks, ids=batch_ids)

            except Exception as e:
                print(
                    f"!!! YÃœKLEME HATASI BATCH {int(i/MAX_BATCH_SIZE) + 1} !!! Hata: {e}"
                )
                # Hata durumunda dÃ¶ngÃ¼den Ã§Ä±kÄ±labilir veya hata loglanÄ±p devam edilebilir
                break

    print(f"\n--- YÃ¼kleme Ã–zeti ---")
    print(f"Toplam ParÃ§a Ä°ÅŸlendi: {len(all_chunks)}")
    print(f"âœ… Yeni Eklendi: {new_count} adet parÃ§a.")
    print(f"ğŸ”„ GÃ¼ncellendi (Overwrite Edildi): {updated_count} adet parÃ§a.")
    print("VeritabanÄ± baÅŸarÄ±yla gÃ¼ncellendi (Batching KullanÄ±ldÄ±).")

    return vectorstore


def test(vectorstore):
    query = "Cem YÄ±lmaz'Ä±n oynadÄ±ÄŸÄ± karakterin logar ile ilgili ilginÃ§ bir sÃ¶zÃ¼ neydi?"

    # En alakalÄ± 3 adet parÃ§ayÄ± (chunk) getir.
    retrieved_docs = vectorstore.similarity_search(query, k=3)

    print(f"\nSorgu: '{query}'")
    print("-" * 40)
    print(f"ChromaDB'den Gelen {len(retrieved_docs)} En AlakalÄ± ParÃ§a:")

    for i, doc in enumerate(retrieved_docs):
        print(f"\n--- ParÃ§a {i+1} ---")
        print(f"Kaynak: {doc.metadata.get('source', 'Bilinmiyor')}")
        print(doc.page_content[:250] + "...")


if __name__ == "__main__":
    vectorstore = db_update()
