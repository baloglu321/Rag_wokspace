import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from chromadb.errors import NotFoundError
import hashlib
import os

# --- Ayarlar ---
CHROMA_HOST = "localhost" # Sadece ana bilgisayar adÄ±
CHROMA_PORT = 8000        # Sadece port numarasÄ±
COLLECTION_NAME = "gora_arog_rag_koleksiyonu"
FILE_PATH = "/home/mbaloglu/Rag/database/" # Kendi dosyanÄ±zÄ±n adÄ±


def db_update():
   
    # 4. ChromaDB Ä°stemcisi OluÅŸturma ve BaÄŸlanma
    client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
    print(f"Eski koleksiyon ({COLLECTION_NAME}) siliniyor...")
    try:
            # Silme iÅŸlemini dene
            client.delete_collection(COLLECTION_NAME) 
            print("Silme baÅŸarÄ±lÄ±.")
    except NotFoundError:
            # EÄŸer koleksiyon yoksa (ilk Ã§alÄ±ÅŸtÄ±rma), hatayÄ± yoksay ve devam et
            print("Koleksiyon zaten mevcut deÄŸil. Yeni koleksiyon oluÅŸturulacak.")
    # 5. Chroma Veri Deposu OluÅŸturma (BaÄŸlantÄ±lÄ±)
    # VektÃ¶rler bu koleksiyona yÃ¼klenir.
    vectorstore=update_db_with_feedback(FILE_PATH,client=client,collection_name=COLLECTION_NAME)
        
    print(f"VektÃ¶rler baÅŸarÄ±yla Docker sunucusundaki '{COLLECTION_NAME}' koleksiyonuna yÃ¼klendi.")
    # 5. Sorgu HazÄ±rlama (Retrieval)
    return vectorstore

def create_id(chunk_content):
    """Metin iÃ§eriÄŸinden kalÄ±cÄ± bir SHA256 hash ID'si oluÅŸturur."""
    return hashlib.sha256(chunk_content.encode('utf-8')).hexdigest()


def get_chunks_with_ids(file_dir_path: str, chunk_size: int = 2000, chunk_overlap: int = 200):
    """Veri dizinindeki tÃ¼m dosyalarÄ± yÃ¼kler, parÃ§alar ve her parÃ§aya iÃ§erik tabanlÄ± ID atar."""
    
    files = os.listdir(file_dir_path)
    all_documents = []
    
    for f in files:
        file = os.path.join(file_dir_path, f)
        try:
            loader = TextLoader(file, encoding="utf8")
            all_documents.extend(loader.load())
        except Exception as e:
            print(f"UYARI: {f} yÃ¼klenirken hata oluÅŸtu: {e}")
            continue

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", " ", ""]
    )
    all_chunks = text_splitter.split_documents(all_documents)
    
    ids = []
    for chunk in all_chunks:
        # Metin iÃ§eriÄŸine baÄŸlÄ± ID oluÅŸturma
        chunk_id = hashlib.sha256(chunk.page_content.encode('utf-8')).hexdigest()
        ids.append(chunk_id)

    return all_chunks, ids

def update_db_with_feedback(file_dir_path: str, client: chromadb.HttpClient, collection_name: str):
    """
    VeritabanÄ±nÄ± gÃ¼nceller, mevcut parÃ§alarÄ± kontrol eder ve geri bildirim saÄŸlar.
    (Mevcut parÃ§alar overwrite edilir, silinmez).
    """
    
    # Veriyi hazÄ±rlama
    all_chunks, ids = get_chunks_with_ids(file_dir_path)
    print(f"HazÄ±rlanan toplam parÃ§a sayÄ±sÄ±: {len(all_chunks)}")

    embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # VektÃ¶r deposunu al (EÄŸer koleksiyon yoksa oluÅŸturur)
    vectorstore = Chroma(
        client=client,
        collection_name=collection_name,
        embedding_function=embedding_function
    )
    
    # ------------------ DURUM KONTROLÃœ VE YÃœKLEME ------------------
    
    # ChromaDB'ye parÃ§a parÃ§a ekleme/gÃ¼ncelleme (Daha fazla kontrol iÃ§in)
    # LangChain'in add_documents'u arka planda ID'ye gÃ¶re gÃ¼ncelleme (overwrite) yapar.
    
    # Mevcut tÃ¼m ID'leri Ã§ekelim (Sadece durum kontrolÃ¼ iÃ§in)
    try:
        existing_ids = client.get_collection(collection_name).get(include=[])['ids']
        existing_ids_set = set(existing_ids)
    except Exception:
        # Koleksiyon ilk kez oluÅŸturuluyorsa
        existing_ids_set = set()
    
    chunks_to_add = []
    ids_to_add = []
    
    # Hangi parÃ§alarÄ±n yeni, hangilerinin gÃ¼ncelleneceÄŸini belirleme (loglama iÃ§in)
    new_count = 0
    updated_count = 0
    
    for i, chunk_id in enumerate(ids):
        if chunk_id in existing_ids_set:
            # ID zaten var, bu OVERWRITE (gÃ¼ncelleme) olacak
            updated_count += 1
            # GÃ¼ncelleme de ekleme iÅŸlemiyle aynÄ±dÄ±r
            chunks_to_add.append(all_chunks[i])
            ids_to_add.append(chunk_id)
        else:
            # ID yok, bu yeni bir ekleme olacak
            new_count += 1
            chunks_to_add.append(all_chunks[i])
            ids_to_add.append(chunk_id)

    # YÃ¼kleme (LangChain'in add_documents metodu hem yeni ekler hem de var olan ID'leri gÃ¼nceller)
    if chunks_to_add:
        # Bu iÅŸlem arka planda hem yeni ekler hem de eski ID'leri gÃ¼nceller (overwrite)
        vectorstore.add_documents(documents=chunks_to_add, ids=ids_to_add)

    print(f"\n--- YÃ¼kleme Ã–zeti ---")
    print(f"Toplam ParÃ§a Ä°ÅŸlendi: {len(all_chunks)}")
    print(f"âœ… Yeni Eklendi: {new_count} adet parÃ§a.")
    print(f"ğŸ”„ GÃ¼ncellendi (Overwrite Edildi): {updated_count} adet parÃ§a.")
    print("VeritabanÄ± baÅŸarÄ±yla gÃ¼ncellendi.")
    
    return vectorstore



def test(vectorstore):
    query = "Cem YÄ±lmaz'Ä±n oynadÄ±ÄŸÄ± karakterin logar ile ilgili ilginÃ§ bir sÃ¶zÃ¼ neydi?"

    # En alakalÄ± 3 adet parÃ§ayÄ± (chunk) getir.
    retrieved_docs = vectorstore.similarity_search(query, k=3)

    print(f"\nSorgu: '{query}'")
    print("-" * 40)
    print(f"ChromaDB'den Gelen {len(retrieved_docs)} En AlakalÄ± ParÃ§a:")

    for i, doc in enumerate(retrieved_docs):
        print(f"\n--- ParÃ§a {i+1} ---")
        print(f"Kaynak: {doc.metadata.get('source', 'Bilinmiyor')}")
        print(doc.page_content[:250] + "...")

if __name__ == "__main__":
    vectorstore=db_update()
    
