import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from chromadb.errors import NotFoundError
from langchain_community.retrievers import BM25Retriever
import hashlib
import os
import json

# --- Ayarlar ---
CHROMA_HOST = "localhost"  # Sadece ana bilgisayar adƒ±
CHROMA_PORT = 8000  # Sadece port numarasƒ±
COLLECTION_NAME = "rag_test_data"
FILE_PATH = "/home/mbaloglu/Rag/database/"  # Kendi dosyanƒ±zƒ±n adƒ±
MAX_BATCH_SIZE = 5000


def db_update():

    # 4. ChromaDB ƒ∞stemcisi Olu≈üturma ve Baƒülanma
    client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
    print(f"Eski koleksiyon ({COLLECTION_NAME}) siliniyor...")
    try:
        # Silme i≈ülemini dene
        client.delete_collection(COLLECTION_NAME)
        print("Silme ba≈üarƒ±lƒ±.")
    except NotFoundError:
        # Eƒüer koleksiyon yoksa (ilk √ßalƒ±≈ütƒ±rma), hatayƒ± yoksay ve devam et
        print("Koleksiyon zaten mevcut deƒüil. Yeni koleksiyon olu≈üturulacak.")
    # 5. Chroma Veri Deposu Olu≈üturma (Baƒülantƒ±lƒ±)
    # Vekt√∂rler bu koleksiyona y√ºklenir.
    vectorstore, bm25_retriever = update_db_with_feedback(
        FILE_PATH, client=client, collection_name=COLLECTION_NAME, bm_ret=False
    )

    print(
        f"Vekt√∂rler ba≈üarƒ±yla Docker sunucusundaki '{COLLECTION_NAME}' koleksiyonuna y√ºklendi."
    )
    # 5. Sorgu Hazƒ±rlama (Retrieval)
    return vectorstore


def create_id(chunk):
    """Metin i√ßeriƒüi, kaynak dosyasƒ± ve metin sƒ±rasƒ±nƒ± kullanarak benzersiz ID olu≈üturur."""
    # Kaynak dosyasƒ± (SQuAD verisinde: dosya adƒ± + ba≈ülƒ±k)
    source = chunk.metadata.get("source", "") + chunk.metadata.get("title", "")

    # Metin i√ßeriƒüi
    content = chunk.page_content

    # Metin + Kaynak bilgisini birle≈ütirip hash alƒ±yoruz
    unique_string = f"{source}_{content}"

    return hashlib.sha256(unique_string.encode("utf-8")).hexdigest()


def get_chunks_with_ids(
    file_dir_path: str, chunk_size: int = 500, chunk_overlap: int = 50
):
    """Veri dizinindeki t√ºm JSON dosyalarƒ±nƒ± okur, SQuAD formatƒ±nƒ± √ß√∂zer ve par√ßalara ayƒ±rƒ±r."""

    all_documents = []

    for filename in os.listdir(file_dir_path):
        if filename.endswith(".json"):
            file_path = os.path.join(file_dir_path, filename)
            print(f"-> {filename} dosyasƒ± okunuyor...")

            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # SQuAD JSON yapƒ±sƒ±nƒ± gezme
            # Yapƒ±: data -> paragraphs -> context
            for item in data.get("data", []):
                for paragraph_data in item.get("paragraphs", []):
                    # Her bir paragrafƒ± (context), t√ºm SQuAD veri setinin ana baƒülamƒ±nƒ± temsil eden
                    # tek bir belge olarak alƒ±yoruz.
                    context_text = paragraph_data.get("context")
                    title = item.get("title", "Bilinmeyen Ba≈ülƒ±k")

                    if context_text:
                        # LangChain Document nesnesi olu≈üturma
                        all_documents.append(
                            Document(
                                page_content=context_text,
                                metadata={"source": file_path, "title": title},
                            )
                        )

    # 2. B√∂lme (Chunking) - Artƒ±k par√ßalama i≈ülemini b√ºy√ºk paragraflar √ºzerinde yapƒ±yoruz
    print(
        f"Toplam {len(all_documents)} paragraf/belge hazƒ±rlandƒ±. Par√ßalara ayrƒ±lƒ±yor..."
    )
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""],
    )
    all_chunks = text_splitter.split_documents(all_documents)

    # 3. ID Olu≈üturma ve Ayƒ±rma
    ids = []
    for i, chunk in enumerate(all_chunks):  # enumerate ile sƒ±rasƒ±nƒ± (i) alƒ±yoruz
        # Metin i√ßeriƒüine sadece i (index) ekleyerek benzersizliƒüi garanti ediyoruz
        unique_string = f"{chunk.page_content}_{i}"
        chunk_id = hashlib.sha256(unique_string.encode("utf-8")).hexdigest()

        ids.append(chunk_id)
        chunk.metadata["ids"] = chunk_id

    return all_chunks, ids


def update_db_with_feedback(
    file_dir_path: str, client: chromadb.HttpClient, collection_name: str, bm_ret=False
):
    """
    Veritabanƒ±nƒ± g√ºnceller, mevcut par√ßalarƒ± kontrol eder ve geri bildirim saƒülar.
    (Mevcut par√ßalar overwrite edilir, silinmez).
    """

    # Veriyi hazƒ±rlama
    all_chunks, ids = get_chunks_with_ids(file_dir_path)
    print(f"Hazƒ±rlanan toplam par√ßa sayƒ±sƒ±: {len(all_chunks)}")

    embedding_function = HuggingFaceEmbeddings(
        model_name="paraphrase-multilingual-mpnet-base-v2"
    )

    # Vekt√∂r deposunu al (Eƒüer koleksiyon yoksa olu≈üturur)
    vectorstore = Chroma(
        client=client,
        collection_name=collection_name,
        embedding_function=embedding_function,
    )

    # ------------------ DURUM KONTROL√ú VE Y√úKLEME ------------------

    # ChromaDB'ye par√ßa par√ßa ekleme/g√ºncelleme (Daha fazla kontrol i√ßin)
    # LangChain'in add_documents'u arka planda ID'ye g√∂re g√ºncelleme (overwrite) yapar.

    # Mevcut t√ºm ID'leri √ßekelim (Sadece durum kontrol√º i√ßin)
    try:
        existing_ids = client.get_collection(collection_name).get(include=[])["ids"]
        existing_ids_set = set(existing_ids)
    except Exception:
        # Koleksiyon ilk kez olu≈üturuluyorsa
        existing_ids_set = set()

    chunks_to_add = []
    ids_to_add = []

    # Saya√ßlar
    new_count = 0
    skipped_count = 0  # G√ºncelleme yerine "Atlanan" sayacƒ±

    for i, chunk_id in enumerate(ids):
        if chunk_id in existing_ids_set:
            # HATANIN KAYNAƒûI BURASIYDI:
            # Eskiden buraya ekliyordun, ≈üimdi sadece sayacƒ± artƒ±rƒ±p ge√ßiyoruz.
            skipped_count += 1
            continue  # Listeye eklemeden bir sonraki d√∂ng√ºye ge√ß
        else:
            # ID yok, bu ger√ßekten yeni bir veri
            new_count += 1
            chunks_to_add.append(all_chunks[i])
            ids_to_add.append(chunk_id)

    # Y√ºkleme Kƒ±smƒ±
    # Eƒüer eklenecek yeni par√ßa varsa batch i≈ülemine gir
    if chunks_to_add:
        print(f"üöÄ {len(chunks_to_add)} yeni par√ßa tespit edildi, y√ºkleniyor...")

        # ... Batch d√∂ng√ºs√º (senin kodunla aynƒ±) ...
        num_chunks = len(chunks_to_add)
        for i in range(0, num_chunks, MAX_BATCH_SIZE):
            end_index = min(i + MAX_BATCH_SIZE, num_chunks)
            batch_chunks = chunks_to_add[i:end_index]
            batch_ids = ids_to_add[i:end_index]

            print(
                f"   -> Batch {int(i/MAX_BATCH_SIZE) + 1}: {len(batch_chunks)} par√ßa y√ºkleniyor..."
            )
            try:
                vectorstore.add_documents(documents=batch_chunks, ids=batch_ids)
            except Exception as e:
                print(f"!!! HATA: {e}")
                break
    else:
        print("‚ú® Eklenecek yeni veri yok. Veritabanƒ± g√ºncel.")

    print(f"\n--- Y√ºkleme √ñzeti ---")
    print(f"Toplam Kaynak Par√ßa: {len(all_chunks)}")
    print(f"‚è≠Ô∏è  Atlanan (Zaten Var): {skipped_count}")
    print(f"‚úÖ Yeni Eklenen: {new_count}")
    if bm_ret == True:
        bm25_retriever = BM25Retriever.from_documents(all_chunks)
        bm25_retriever.k = 10
    else:
        bm25_retriever = False

    return vectorstore, bm25_retriever


if __name__ == "__main__":
    vectorstore = db_update()
